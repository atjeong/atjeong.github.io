<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Jeonghoon Park</title>
    <link>https://atjeong.github.io/post/</link>
    <description>Recent content in Posts on Jeonghoon Park</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Jul 2022 16:43:25 +0900</lastBuildDate><atom:link href="https://atjeong.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BiasEnsemble: Revisiting the Importance of Amplifying Bias for Debiasing</title>
      <link>https://atjeong.github.io/post/bias_ensemble/</link>
      <pubDate>Mon, 11 Jul 2022 16:43:25 +0900</pubDate>
      
      <guid>https://atjeong.github.io/post/bias_ensemble/</guid>
      <description>Jungsoo Lee*, Jeonghoon Park*, Daeyoung Kim*, Junyoung Lee, Edward Choi, Jaegul Choo ðŸ”— Abstract ðŸ”—In image classification, &amp;ldquo;debiasing&amp;rdquo; aims to train a classifier to be less susceptible to dataset bias, the strong correlation between peripheral attributes of data samples and a target class. For example, even if the frog class in the dataset mainly consists of frog images with a swamp background (i.e., bias-aligned samples), a debiased classifier should be able to correctly classify a frog at a beach (i.</description>
    </item>
    
    <item>
      <title>Deep Edge-Aware Interactive Colorization against Color Bleeding Effects</title>
      <link>https://atjeong.github.io/post/deepedge/</link>
      <pubDate>Sat, 13 Nov 2021 16:00:04 +0900</pubDate>
      
      <guid>https://atjeong.github.io/post/deepedge/</guid>
      <description>ICCV 2021 (Oral) ðŸŽ‰ ðŸ”—Eungyeup Kim*, Sanghyeon Lee*, Jeonghoon Park*, Somi choi, Choonghyun Seo, Jaegul Choo ðŸ”— Abstract ðŸ”—Deep image colorization networks often suffer from the color-bleeding artifact, a problematic color spreading near the boundaries between adjacent objects. The color-bleeding artifacts debase the reality of generated outputs, limiting the applicability of colorization models on a practical application. Although previous approaches have tackled this problem in an automatic manner, they often generate imperfect outputs because their enhancements are available only in limited cases, such as having a high contrast of gray-scale value in an input image.</description>
    </item>
    
    <item>
      <title>Evaluation of Out-of-Distribution Detection Performance of Self-Supervised Learning in a Controllable Environment</title>
      <link>https://atjeong.github.io/post/ood_ssl/</link>
      <pubDate>Fri, 13 Nov 2020 16:43:02 +0900</pubDate>
      
      <guid>https://atjeong.github.io/post/ood_ssl/</guid>
      <description>NeurIPS 2020 Workshop: Self-Supervised Learning - Theory and Practice ðŸ”—Jeonghoon Park*, Kyungmin Jo*, Daehoon Gwak*, Jimin Hong, Jaegul Choo, Edward Choi ðŸ”— Abstract ðŸ”—We evaluate the out-of-distribution (OOD) detection performance of self-supervised learning (SSL) techniques with a new evaluation framework. Unlike the previous evaluation methods, the proposed framework adjusts the distance of OOD samples from the in-distribution samples. We evaluate an extensive combination of OOD detection algorithms on three different implementations of the proposed framework using simulated samples, images, and text.</description>
    </item>
    
  </channel>
</rss>
